# -*- coding: utf-8 -*-
"""ML_Project_Used_Car_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103XBp2Aih8iQkVRhb5VPbYkVD_OW6n9i
"""

from google.colab import files
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split
import lightgbm as lgbm
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

uploaded = files.upload()

df1 = pd.read_csv("audi.csv")
df1["brand"] = "Audi"
df2 = pd.read_csv("bmw.csv")
df2["brand"] = "BMW"
df3 = pd.read_csv("ford.csv")
df3["brand"] = "Ford"
df4 = pd.read_csv("hyundi.csv")
df4["brand"] = "Hyundi"
df5 = pd.read_csv("merc.csv")
df5["brand"] = "Merc"
df6 = pd.read_csv("skoda.csv")
df6["brand"] = "Skoda"
df7 = pd.read_csv("toyota.csv")
df7["brand"] = "Toyota"
df7 = pd.read_csv("vw.csv")
df7["brand"] = "VW"

df  = pd.concat([df1, df2, df3, df4, df5, df6, df7])

df.head()

df = df.drop(columns=['model','tax','tax(Â£)'])

df.head()

current_year = datetime.now().year
df["car_age"] = current_year - df["year"]
df = df.drop(columns=["year"])

df.transmission.unique()

transmission_map = {
    'Manual': 0,
    'Automatic': 1,
    'Semi-Auto': 2,
    'Other': 3
}

df["transmission"] = df["transmission"].map(transmission_map)

df.fuelType.unique()

fuelType_map = {
    'Petrol': 0,
    'Diesel': 1,
    'Hybrid': 2,
    'Other': 3,
    'Electric': 4
}

df["fuelType"] = df["fuelType"].map(fuelType_map)

df.brand.unique()

brand_map = {
    'Audi': 0,
    'BMW': 1,
    'Ford': 2,
    'Hyundi': 3,
    'Merc': 4,
    'Skoda': 5,
    'VW': 6
}

df["brand"] = df["brand"].map(brand_map)

df.head()

X = df.drop(columns=["price"])
y = df["price"]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

train_data = lgbm.Dataset(X_train, label=y_train, categorical_feature=['transmission', 'fuelType', 'brand'])
val_data = lgbm.Dataset(X_val, label=y_val, reference=train_data, categorical_feature=['transmission', 'fuelType', 'brand'])

params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',         # Gradient Boosting Decision Tree (default and good)
    'num_leaves': 31,                # Controls tree complexity, 31 is a good start
    'learning_rate': 0.05,           # Step size shrinkage, smaller means slower but better training
    'feature_fraction': 0.9,         # Use 90% of features in each iteration (helps prevent overfitting)
    'bagging_fraction': 0.8,         # Use 80% of data in each iteration (helps generalization)
    'bagging_freq': 5,               # Bagging performed every 5 iterations
    'verbose': -1                   # Suppress info logs during training
}
num_round = 10000
bst = lgbm.train(params, train_data, num_round, valid_sets=[val_data], callbacks=[lgbm.early_stopping(stopping_rounds=100)])
bst.save_model('model.txt', num_iteration=bst.best_iteration)

y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)

# Compute RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse:.4f}")

df['price'].mean()

!pip install fastapi

import lightgbm as lgb
from fastapi import FastAPI
from pydantic import BaseModel

model = lgb.Booster(model_file='model.txt')

app = FastAPI()

# Define the input schema
class CarFeatures(BaseModel):
    transmission: float
    mileage: float
    fuelType: float
    mpg: float
    engineSize: float
    brand: float
    car_age: float

@app.post("/predict")
def predict(features: CarFeatures):
    data = np.array([[features.transmission, features.mileage, features.fuelType, features.mpg, features.engineSize, features.brand, features.car_age]])
    prediction = model.predict(data)
    return {"prediction": prediction[0]}

from google.colab import files
files.download('model.txt')
